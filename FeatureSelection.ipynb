{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOwdpq2ArSh7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feeture Selection\n",
        "\n",
        "\n",
        "## Mutual Information\n",
        "\n",
        "*   **sklearn.feature_selection.mutual_info_classif**\n",
        "\n",
        "It calculates the Mutual Information between the features (independent variables) and the target variable (dependent variable) in a classification problem\n",
        "\n",
        "Mutual Information is a measure of the statistical dependence or information shared between two random variables\n",
        "\n",
        "Input:\n",
        "\n",
        "X: This is the feature matrix, a 2D array-like structure where each row represents a sample, and each column represents a feature.\n",
        "y: This is the target variable, a 1D array or list containing the class labels or target values for the corresponding samples in X.\n",
        "\n",
        "After calculating the mutual information for each feature, mutual_info_classif returns a list of scores, one for each feature.\n",
        "Higher scores indicate that a feature has more mutual information with the target variable, implying that it might be more important in making classification decisions.\n",
        "\n",
        "$$I\\left[X;Y\\right]=\\sum_{y\\in Y}\\sum_{x\\in X}p\\left[x,y\\right]\\log\\left(\\frac{p\\left[x,y\\right]}{p\\left[x\\right]p\\left[y\\right]}\\right)$$\n",
        "\n",
        "If Y and X are independent, is cannot find Y from X, so the Mutual information is: $I\\left[X;Y\\right]=0$ because: $\\log{\\left(\\frac{p\\left[x,y\\right]}{p\\left[x\\right]p\\left[y\\right]}\\right)}=\\log{1}=0$\n",
        "\n",
        "Si a determinist function exist you can get Y from X, so the mutual information will be: $I\\left[X;Y\\right]=1$.\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# X is your feature matrix, and y is your target variable\n",
        "# Calculate mutual information scores for each feature\n",
        "mi_scores = mutual_info_classif(X, y)\n",
        "\n",
        "# Select the top-k features based on their mutual information scores\n",
        "top_k_features = X[:, mi_scores.argsort()[::-1][:k]]\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ri5pDE3ZrTRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectPercentile\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "# cancer is of type Bunch (Dictionary-like object)\n",
        "cancer = load_breast_cancer ()\n",
        "X = cancer['data']\n",
        "y = cancer['target']\n",
        "\n",
        "#print(cancer)\n",
        "\n",
        "# Set 1: train and test datasets (using all data).\n",
        "X_train_1,X_test_1,y_train_1,y_test_1 = train_test_split(X,y,random_state=0,stratify=y)\n",
        "\n",
        "print(\"Datos de entrenamiento con todas las característica\", X_train_1.shape)\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "# Compute MI (mutual information) score\n",
        "\n",
        "transformer = MinMaxScaler().fit(X)\n",
        "X = transformer.transform(X)\n",
        "#print( X[0:5] )\n",
        "\n",
        "# Vector with mutual information scores\n",
        "mi_score = mutual_info_classif(X,y)\n",
        "print(mi_score)\n",
        "\n",
        "# Since there are 30 characteristics, the algorithm generates\n",
        "# a vector with scores assigned to each one.\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Select the top-k features based on their mutual information scores\n",
        "k  = 1\n",
        "top_k_features = X[:, mi_score.argsort()[::-1][:k]]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hx47bgo7tZQm",
        "outputId": "a0604bea-4017-4afe-e564-9bb7b384157f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos de entrenamiento con todas las característica (426, 30)\n",
            "\n",
            "\n",
            "[0.36973092 0.10297018 0.4036949  0.36105915 0.07514198 0.21020062\n",
            " 0.37300064 0.43997809 0.06489205 0.00863471 0.24943928 0.00077096\n",
            " 0.27593128 0.33850185 0.01648752 0.0751082  0.11767169 0.12332735\n",
            " 0.0133077  0.04037909 0.45154883 0.1252039  0.4779135  0.46387392\n",
            " 0.10034046 0.22521388 0.31537009 0.43764105 0.09245879 0.0697103 ]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can define a threshold, next cases will be 0.2.\n",
        "For this example, you will have 15 attributes with more mutual information. it will reduce the current dataset in 15 feetures"
      ],
      "metadata": {
        "id": "898_lWgWufUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 2: with features having MI scores > 0.2\n",
        "\n",
        "# Characteristics with mi_score > 0.2\n",
        "\n",
        "mi_score_selected_index = np.where(mi_score >0.2)[0]\n",
        "print(mi_score_selected_index)\n",
        "\n",
        "# X_2 data with features having MI scores > 0.2\n",
        "X_2 = X[:,mi_score_selected_index]\n",
        "\n",
        "X_train_2,X_test_2,y_train,y_test = train_test_split(X_2,y,random_state=0,stratify=y)\n",
        "\n",
        "print(\"Datos de entrenamiento con características con peso de información mutua > 0.2\", X_train_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPhhk2mNuVAG",
        "outputId": "fd4e6c31-7de9-4312-a82c-e9d613ea5509"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  2  3  5  6  7 10 12 13 20 22 23 25 26 27]\n",
            "Datos de entrenamiento con características con peso de información mutua > 0.2 (426, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example you take the features with less MI"
      ],
      "metadata": {
        "id": "wMjb_mZ-u6-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 3 with features have MI (mutual information) scores less than 0.2\n",
        "mi_score_selected_index = np.where(mi_score <= 0.2)[0]\n",
        "print(mi_score_selected_index)\n",
        "\n",
        "X_3 = X[:,mi_score_selected_index]\n",
        "X_train_3,X_test_3,y_train,y_test = train_test_split(X_3,y,random_state=0,stratify=y)\n",
        "\n",
        "print(\"Datos de entrenamiento con características con peso de información mutua < 0.2\", X_train_3.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qWERNtXu7ph",
        "outputId": "b8bc5997-5eda-461d-ed00-10791074cdec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  4  8  9 11 14 15 16 17 18 19 21 24 28 29]\n",
            "Datos de entrenamiento con características con peso de información mutua < 0.2 (426, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use a Decision Tree to Test the Features selection performance.\n",
        "For this example is better to use the 15 best features (according to the MI) in comparison with the whole Dataset and the lower MI features.\n"
      ],
      "metadata": {
        "id": "w3mPG1ZVvIQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test classifiers, one for each dataset with different columns.\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_1 = DecisionTreeClassifier().fit(X_train_1,y_train)\n",
        "model_2 = DecisionTreeClassifier().fit(X_train_2,y_train)\n",
        "model_3 = DecisionTreeClassifier().fit(X_train_3,y_train)\n",
        "\n",
        "# Return the mean accuracy on the given test data and labels.\n",
        "score_1 = model_1.score(X_test_1,y_test)\n",
        "score_2 = model_2.score(X_test_2,y_test)\n",
        "score_3 = model_3.score(X_test_3,y_test)\n",
        "print(f\"score_1:{score_1}\\n score_2:{score_2}\\n score_3:{score_3}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wvJMdUyvIXr",
        "outputId": "df203155-68f0-4bdb-ada7-875aafa51798"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score_1:0.916083916083916\n",
            " score_2:0.9300699300699301\n",
            " score_3:0.8251748251748252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automated Way to Calculate the MI\n",
        "This a more Automated way to Calculate the MI scores:"
      ],
      "metadata": {
        "id": "VSJ2FmRJ04o6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector = SelectPercentile(percentile=50) # select features with top 50%\n",
        "selector.fit(X,y)\n",
        "X_4 = selector.transform(X)\n",
        "\n",
        "X_train_4,X_test_4,y_train,y_test = train_test_split(X_4,y,random_state=0,stratify=y)\n",
        "\n",
        "model_4 = DecisionTreeClassifier().fit(X_train_4,y_train)\n",
        "score_4 = model_4.score(X_test_4,y_test)\n",
        "\n",
        "print(f\"score_4:{score_4}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lumfBAPG08uz",
        "outputId": "d6362f03-21b6-45df-8bd6-224933e19ffd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "score_4:0.9300699300699301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression problems:\n",
        "\n",
        "The next information is from Chat GPT\n",
        "\n",
        "\n",
        "1.   F-Regression:\n",
        "\n",
        "F-regression measures the dependence between each feature and the continuous target variable in a regression problem.\n",
        "You can use the sklearn.feature_selection.f_regression function in scikit-learn to compute F-statistic scores and p-values for each feature's relationship with the target variable.\n",
        "Features with higher F-statistic scores and lower p-values are more likely to be informative for regression.\n",
        "\n",
        "2.   Mutual Information for Regression:\n",
        "\n",
        "There's a variant of Mutual Information specifically designed for regression tasks called \"Mutual Information Regression\" (MIR).\n",
        "You can use sklearn.feature_selection.mutual_info_regression in scikit-learn to calculate mutual information scores between continuous features and a continuous target variable.\n",
        "This is more appropriate for regression tasks compared to mutual_info_classif, which is designed for classification.\n",
        "\n",
        "3.   LASSO Regression (L1 Regularization):\n",
        "\n",
        "LASSO (Least Absolute Shrinkage and Selection Operator) is a regularization technique that can be used for feature selection in regression.\n",
        "It encourages some feature coefficients to be exactly zero, effectively performing feature selection.\n",
        "You can use the sklearn.linear_model.Lasso or sklearn.linear_model.LassoCV classes in scikit-learn for LASSO regression.\n",
        "\n",
        "4.   Recursive Feature Elimination (RFE):\n",
        "\n",
        "RFE is an iterative feature selection method that works well for regression tasks.\n",
        "It starts with all features and iteratively removes the least important ones based on the model's performance.\n",
        "You can use the sklearn.feature_selection.RFE class in scikit-learn for this purpose.\n"
      ],
      "metadata": {
        "id": "cHidmtw8zNHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VarianceThreshold\n",
        "\n",
        "This can be used with Unsupervised Problems (you don't need the Y or Target Variable)\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=.1)\n",
        "X_new = selector.fit_transform(X)\n",
        "print(\"varianzas: \", selector.variances_ )\n",
        "print( X_new[0:5] )\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "UyPGvdtg1Yec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More Research to be Done"
      ],
      "metadata": {
        "id": "gVmXu-JX158m"
      }
    }
  ]
}